---
title: spark-sql
excerpt: spark2.4的源码解析
layout: post
categories: Spark
---

{% include toc %}


## 前言
spark sql的代码非常优雅,可以说是scala better java风格的最佳实践范例.
整个代码看下来,第一印象就是成体系.
plan,expression,parser,stats.每一个部分都有明显的边界和简洁的接口.

## spark sql入口
```
SQLContext.sql(sqlText: String)-->SparkSession.sql(sqlText: String)
```
## spark sql流程
```
sql-->
unresolve逻辑执行计划-->
resolve逻辑执行计划-->
逻辑执行计划优化-->
物理执行计划-->
物理执行计划优化.
```
## 解析unresolve逻辑执行计划
```
- 入口：(SparkSession.sql(sqlText: String))
val plan = tracker.measurePhase(QueryPlanningTracker.PARSING) {
    sessionState.sqlParser.parsePlan(sqlText)
}

实际的解析入口：
SparkSqlParser.parsePlan(sqlText: String): LogicalPlan
返回unresolve逻辑执行计划
```
```
- 实现(主要逻辑在SparkSqlParser的父类AbstractSqlParser)
1.语法解析,获取语法树
使用antlr4，根据g4文件，生成语法解析代码。
(lexer,listener,parser,visitor)
用antlr4的lexer,listener,parser解析sql语法,获取语法树。
代码逻辑在protected def parse[T](command: String)(toResult: SqlBaseParser => T): T
2.根据语法树，生成unreolve逻辑执行计划
继承重写antlr4生成的SqlBaseBaseVisitor-->AstBuilder.
AstBuilder用visit模式遍历语法树，转换LogicalPlan(unreolve)
`astBuilder.visitSingleStatement(parser.singleStatement())`
- visit模式
antlr4的visitor调用visit方法-->parseTree.accept(visitor)-->具体到哪个语法树节点，就有对应的visitXXX方法被调用.
```

## resolve逻辑执行计划
```
- 入口
Dataset.ofRows(self, plan, tracker)-->
new QueryExecution(sparkSession, logicalPlan, tracker).assertAnalyzed()-->
sparkSession.sessionState.analyzer.executeAndCheck(logical, tracker)-->

实际入口:
Analyzer.executeAndCheck(plan: LogicalPlan, tracker: QueryPlanningTracker)
Analyzer是spark sql用于resolve逻辑执行计划的类。
```
```
实现：
Analyzer实际上是继承的RuleExecutor，扩展RuleExecutor的一些接口方法，去实现resolve LogicalPlan.

主要就是实现batches(`lazy val batches: Seq[Batch] = xxx`),
batches是定义的一堆规则,在RuleExecutor定义的规则下，遍历执行计划.

Analyzer resolve语法树就是通过batches里的batcher和batches包含的rule实现类

几个例子:
- CTESubstitution  把cte复写为普通子查询,写法很简练,可以参考一下
- LookupFunctions 用来确认function是否存在，简单例子
- ResolveAliases 对UnresolvedAlias生成别名,默认用sql当别名,`e => Alias(e, toPrettySQL(e))()`
```
- RuleExecutor和Rule
```
RuleExecutor是rule的执行框架，
batches: Seq[Batch]，依次遍历batches
batches则定义了一组rule和batch里rule执行次数
一个batch结束才会到下一个batch

Rule是一个规则的实现，接口很简单，就一个apply。
理论上怎么使用都行，但是spark给了遍历plan的一组方法，rule实现经常用，如：
- resolveOperators 遍历plan
- transformExpressions 遍历plan里的expression
```
## optimize逻辑执行计划
```
- 入口：
class QueryExecution {
    lazy val sparkPlan: SparkPlan = tracker.measurePhase(QueryPlanningTracker.PLANNING) {
        SparkSession.setActiveSession(sparkSession)
        // TODO: We use next(), i.e. take the first plan returned by the planner, here for now,
        //       but we will implement to choose the best plan.
        // Clone the logical plan here, in case the planner rules change the states of the logical plan.
        planner.plan(ReturnAnswer(optimizedPlan.clone())).next()
    }
}
- 实现类SparkOptimizer extends Optimizer extends RuleExecutor
和Analyzer的实现无异，都是扩展Rule，依赖RuleExecutor去替换logical plan的节点
- 一个简单例子：
EliminateSubqueryAliases，消除子查询别名
plan transformUp {
    case SubqueryAlias(_, child) => child
}
```

## 物理执行计划
```
物理执行计划，分为三个阶段：
--> sparkPlan = planner.plan(ReturnAnswer(optimizedPlan.clone())).next()
--> executedPlan = prepareForExecution(sparkPlan.clone())
--> executedPlan.execute()
这三个阶段都在QueryExecution中进行，最后生成SQLExecutionRDD
整个启动入口是:queryExecution.toRdd
```

- sparkPlan
```
把优化后的LogicalPlan转换为SparkPlan
(SparkPlanner extends SparkStrategies extends QueryPlanner).plan(logicalPlan)生成
依赖QueryPlanner.plan(logicalPlan)去遍历logical plan转换为spark plan，通过扩展def strategies: Seq[GenericStrategy[PhysicalPlan]]

strategies的实现在SparkPlanner中。
例子：FileSourceStrategy，用来转换读文件的relation算子
注：HiveTableRelation被RelationConversions转换为LogicalRelation(fsRelation: HadoopFsRelation, _, table, _)，作为读hdfs文件的LogicalRelation.

planLater是SparkPlan的一个实现，只是作为占位符用的。
从代码上，感觉是用来切分LogicalPlan转SparkPlan的过程。
因为LogicalPlan转SparkPlan只会转到planLater为止，
然后再对每个planLater，转planLater.plan为SparkPlan
```
- prepareForExecution
```
这一阶段是对前一步生成的spark plan做一些准备工作。
QueryExecution.prepareForExecution(plan)
通过扩展protected def preparations: Seq[Rule[SparkPlan]] = Seq(xxx)

preparations的实现在QueryExecution中
例子：PlanSubqueries,用来把plan.subqueries中的子查询转换为物理执行计划，如where dt in(select dt from xxx)
```

- executedPlan.execute()
    - 这一步就直接生成RDD[InternalRow]了.
    - SparkPlan.execute()调用doExecute
    - doExecute的实现一般是调用child.execute()获取childRDD, 再对childRDD调用操作，返回操作后的RDD，这里就是spark-core的方法了.



## SparkPlan的metric体系
```
def metrics: Map[String, SQLMetric] = Map.empty
overwrite metrics，在上面add自己的SQLMetric。
如：
override lazy val metrics = Map(
    "numOutputRows" -> SQLMetrics.createMetric(sparkContext, "number of output rows"))

注册的SQLMetric的运作流程：
--> SQLMetrics.createMetric(SparkContext,name)
--> new SQLMetric().register(SparkContext, Some(name)
--> AccumulatorContext.register(SQLMetric)
--> originals.putIfAbsent(a.id, new jl.ref.WeakReference[AccumulatorV2[_, _]](a))
--> 最终会放到AccumulatorContext的一个ConcurrentHashMap中.
--> spark sql的metrics数据是在SparkPlan那里计算的,比如FilterExec就是在doExecute()那里+1

SQLMetrics的发布必须调用SQLMetrics.postDriverMetricUpdates()
SubqueryExec,BroadcastExchangeExec会调用doPrepare()去调用SQLMetrics.postDriverMetricUpdates()
所以目前只有几个SparkPlan发布了SQLMetrics

- 页面展示:
--> SQLMetrics.postDriverMetricUpdates()包装了一个SparkListenerDriverAccumUpdates发送出去
--> SQLAppStatusListener.onDriverAccumUpdates(event: SparkListenerDriverAccumUpdates)获取driver发布的SQLMetrics
--> SQLAppStatusListener.liveExecutionMetrics更新SQLMetrics的对应指标值
--> SQLAppStatusStore调用executionMetrics获取指标值
--> ExecutionPage.render最终展示指标到页面
```

## cacheTable:spark sql的物化视窗
```
- 入口
sparkPlan：CacheTableCommand
sql: cache table as select
CacheTableCommand会同时创建一个temp view和一个cacheTable
- 创建
CacheTable的管理在CacheManager.
CacheManager.cacheQuery()产生一个CachedData(plan: LogicalPlan, cachedRepresentation: InMemoryRelation)，
并保存在private var cachedData = IndexedSeq[CachedData]()
- 使用,物化
withCachedData = cacheManager.useCachedData(analyzed.clone())
useCachedData(plan)会遍历plan中的每一个节点,遇到可以被CachedData中的plan替代的,就把CachedData对应的InMemoryRelation,即物化.
所以CacheTable实际上就是物化视窗.

- cache table as select会创建一个temp view的原因是：
resolve plan的过程会把temp view的plan替换回去，
然后Cache table再把temp view的plan换成InMemoryRelation.
```

## codegen
    - 利用代码生成，生成class对象执行，避免写代码时，因为运行时的不确定条件使用多态（虚函数）导致的开销.
        - cpu预测失效？
    - WholeStageCodegenExec,用于把logical plan执行codegen接口后，生成physical plan
```
例子:projectExec
--> projectExec的doExecute()是，在child.map中创建这么一个Lambda：
--> 生成UnsafeProjection（UnsafeProjection.create(projectList, child.output)），然后把child的RDD的数据put进UnsafeProjection.
-- > UnsafeProjection.create最终会调用 GenerateUnsafeProjection.generate(in, SQLConf.get.subexpressionEliminationEnabled).
--> generate会生成SpecificUnsafeProjection extends ${classOf[UnsafeProjection].getName}的java代码文本,SpecificUnsafeProjection里会把每一个类型写死javaType,而不是用统一的基类来引用每个字段，减少了多态的开销.
--> 再把SpecificUnsafeProjection的java code生成class。
val (clazz, _) = CodeGenerator.compile(code)
clazz.generate(ctx.references.toArray).asInstanceOf[UnsafeProjection]
```

## 向量化执行
    - 入口：preparations: Seq[Rule[SparkPlan]] = Seq(ApplyColumnarRulesAndInsertTransitions(sparkSession.sessionState.conf,
      sparkSession.sessionState.columnarRules))
    - if(plan.supportsColumnar) ColumnarToRowExec(plan)
    - ColumnarToRowExec.doExecute()调用plan.executeColumnar()；plan.supportsColumnar就会实现executeColumnar()

## cbo
    - spark sql的cbo是自己做的,各个算子收集Statistics,然后在join时根据Statistics选择join方式
        - 如果不用cbo，假设两边的表的行数一样多，根据每一行大概占多少字节来判断join两边数据量大小（SizeInBytesOnlyStatsPlanVisitor）
        - 如果用cbo，根据每个算子的统计直方图，初略统计每个算子导出的数据量大小，最后join根据左右算子的导出数据来，来决定join方式（BasicStatsPlanVisitor）
    - JoinSelection 根据joinCondition类型和join两边数据量决定join方式
- Statistics
    -  logical plan实现LogicalPlanStats接口，用于返回Statistics，Statistics收集每一个plan导出数据的统计信息.为spark sql的cbo服务.
    -  直方图：记录了数据中某个范围的数量，如1-2多少行，2-3多少行
```
Statistics(
    sizeInBytes: BigInt, //一共多少字节
    rowCount: Option[BigInt] = None, //一共多少行
    attributeStats: AttributeMap[ColumnStat] = AttributeMap(Nil))
```
```
//如果启动cbo,用BasicStatsPlanVisitor，否则SizeInBytesOnlyStatsPlanVisitor
//两者区别在于BasicStatsPlanVisitor会根据收集的直方图去估算导出的行数
//SizeInBytesOnlyStatsPlanVisitor会假设两边行数一样多,所以SizeInBytesOnlyStatsPlanVisitor基本是以每一行的字节数来决定的
LogicalPlanStats {
    def stats: Statistics = 
    if (conf.cboEnabled) {
      Option(BasicStatsPlanVisitor.visit(self))
    } else {
      Option(SizeInBytesOnlyStatsPlanVisitor.visit(self))
    }
}
```
- SizeInBytesOnlyStatsPlanVisitor
    - 这个的实现很简单，直接看代码就可以知道意思了.
    - 核心算法就是`sizeInBytes = (p.child.stats.sizeInBytes * outputRowSize) / childRowSize`
    - 值得一提的是，多child是把child的sizeInBytes乘起来
- BasicStatsPlanVisitor
    - 这个是支撑spark sql cbo的关键，用直方图估计算子output的字节数
```
- BasicStatsPlanVisitor中除了join,project,aggregate,filter，其他都是调用SizeInBytesOnlyStatsPlanVisitor

- AggregateEstimation和ProjectEstimation主要是对Statistics的attributeStats: AttributeMap[ColumnStat]的设置，不涉及直方图，看代码就可以理解。

- FilterEstimation,根据直方图，估计filter算子返回的数据量大小,后面专门开一个讲
val filterSelectivity = calculateFilterSelectivity(plan.condition).getOrElse(1.0) //计算出filter后的数据占比
val filteredRowCount: BigInt = ceil(BigDecimal(childStats.rowCount.get) * filterSelectivity) //计算filter后的行数
val newColStats = if (filteredRowCount == 0) { //获取新的AttributeMap
    AttributeMap[ColumnStat](Nil)
} else {
    colStatsMap.outputColumnStats(rowsBeforeFilter = childStats.rowCount.get,
    rowsAfterFilter = filteredRowCount)
}
val filteredSizeInBytes: BigInt = getOutputSize(plan.output, filteredRowCount, newColStats) //算出总的字节大小

- JoinEstimation 根据join类型选择不同的估计,后面专门开一个讲
    join.joinType match {
      case Inner | Cross | LeftOuter | RightOuter | FullOuter =>
        estimateInnerOuterJoin()
      case LeftSemi | LeftAnti =>
        estimateLeftSemiAntiJoin()
      case _ =>
        logDebug(s"[CBO] Unsupported join type: ${join.joinType}")
        None
    }
```
- Histogram
    - spark sql直方图结构
```
//height:Histogram里每个bin的row数量，所以直方图里每个bin的row是一样多的。
//bins直方图里每个range的同级信息,如[1,10]的同级信息
class Histogram(height: Double, bins: Array[HistogramBin])

//lo --> min
//hi --> max
//ndv --> disctinct count
class HistogramBin(lo: Double, hi: Double, ndv: Long)
```
- FilterEstimation
    - 关键步骤是：如果根据直方图获取filter后数据量的比率，即：`val filterSelectivity = calculateFilterSelectivity(plan.condition).getOrElse(1.0)`
```
//返回condition过滤后的数据量百分比
calculateFilterSelectivity(ondition: Expression, update: Boolean = true): Option[Double]

//and or连接词处理
//符合概率公式做法
val percent1 = calculateFilterSelectivity(left, update).getOrElse(1.0)
val percent2 = calculateFilterSelectivity(right, update).getOrElse(1.0)
and: percent1*percent2
or: percent1 + percent2 - (percent1 * percent2)

//处理单个条件关系：不包含and，or
calculateSingleCondition(condition: Expression, update: Boolean): Option[Double]
```
- calculateSingleCondition
    - 这个方法用于处理单个条件关系:如=,>,>=等等.这里只分析[变量=常量]的情况.
    - `evaluateEquality(attr: Attribute,literal: Literal,update: Boolean): Option[Double]`
```
evaluateEquality步骤(attr,literal,update):
1.从colStatsMap中获取attr对应的ColumnStat。
ColumnStat包括最大最小值,distinct数量，null数量等等，还有最重要的直方图Histogram
2.ValueInterval(colStat.min, colStat.max, attr.dataType).contains(literal)
判断literal在不在attr的范围内
值得一提的是，数字类型全都转为double(double是数字类型的超集)去判断，而string,byte类型都认为成立.
3.if update,更新ColumnStat,比如最大最小值,distinct数量.
4.计算过滤后数据量的百分比
- 如果ColumnStat没有histogram(直方图),就1.0 / colStat.distinctCount.get.toDouble，假设均匀分布
- 如果有histogram，根据直方图计算比率computeEqualityPossibilityByHistogram(literal, colStat)
```
- computeEqualityPossibilityByHistogram
    - Computes the possibility of an equality predicate using histogram.
```
//获取colStat.min<= x <=colStat.max之间的bin个数
val numBinsHoldingEntireRange = EstimationUtils.numBinsHoldingRange(
      upperBound = max,
      upperBoundInclusive = true,
      lowerBound = min,
      lowerBoundInclusive = true,
      histogram.bins) 
//获取x = literal对应的bin个数
val numBinsHoldingDatum = EstimationUtils.numBinsHoldingRange(
      upperBound = datum,
      upperBoundInclusive = true,
      lowerBound = datum,
      lowerBoundInclusive = true,
      histogram.bins)
//这里用两个range相除，因为每个bin的行数一样大.
return numBinsHoldingDatum / numBinsHoldingEntireRange
```
- `numBinsHoldingRange(upperBound: Double,upperBoundInclusive: Boolean,lowerBound: Double,lowerBoundInclusive: Boolean,bins: Array[HistogramBin]): Double`
    - 获取直方图lowerBound到upperBound之间bin的个数
```
//xxxBoundInclusive是指要不要取闭区间
//获取max对应的bin的下标
val upperBinIndex = if (upperBoundInclusive) {
  findLastBinForValue(upperBound, bins)
} else {
  findFirstBinForValue(upperBound, bins)
}
//获取min对应的bin的下标
val lowerBinIndex = if (lowerBoundInclusive) {
  findFirstBinForValue(lowerBound, bins)
} else {
  findLastBinForValue(lowerBound, bins)
}

//bins(i)获取下标对应的bin
//binHoldingRangePossibility,根据bin的信息，找到min/max在对应的bin中的占比
//binHoldingRangePossibility核心逻辑就是： math.min((upperBound - lowerBound) / (bin.hi - bin.lo), 1.0)
val lowerBin = bins(lowerBinIndex)
val lowerPart = binHoldingRangePossibility(lowerBin.hi, lowerBound, lowerBin)

val higherBin = bins(upperBinIndex)
val higherPart = binHoldingRangePossibility(upperBound, higherBin.lo, higherBin)

//全部加起来就是min到max范围内bin的数量，可能带小数
lowerPart + higherPart + upperBinIndex - lowerBinIndex - 1
```
- JoinEstimation
    - LeftSemi | LeftAnti 
        - estimateLeftSemiAntiJoin()
            - 这个方法就是计算left child的sizeInBytes，和project那些的实现没大差别，直接看代码即可
    - Inner | Cross | LeftOuter | RightOuter | FullOuter
        - estimateInnerOuterJoin()
            - 常规join的处理方法，下面详解
- estimateInnerOuterJoin
```
先占坑，有点复杂
```
- Statistics in DataSource
    - 上面讲的那些都是中间算子如何根据child的`statistics`来计算自己的`statistics`。
    - 但是源头算子，即与`datasouce`相关的算子需要从数据源收集`statistics`。如`HadoopFsRelation`,从hdfs文件收集
```
1.
基本上Relation都被转成了LogicalRelation,如HadoopFsRelation.
  override def computeStats(): Statistics = {
    catalogTable
      .flatMap(_.stats.map(_.toPlanStats(output, conf.cboEnabled)))
      .getOrElse(Statistics(sizeInBytes = relation.sizeInBytes))
  }
2.
从1中可看到,如果从CatalogTable.CatalogStatistics.toPlanStats中拿不到,
就Statistics(sizeInBytes = relation.sizeInBytes)
3.
//HadoopFsRelation.sizeInBytes
//因为hdfs中的文件可能是压缩过的，但是spark中间缓存数据是不压缩的，所以乘以一个factor，这是conf定的，所以不太准确。
sizeInBytes = (location.sizeInBytes * sqlContext.conf.fileCompressionFactor).toLong
4.
//如何获取源头的统计信息
- AnalyzeColumnCommand,sql --> `ANALYZE TABLE table_name COMPUTE STATISTICS`
//获取统计信息的sql命令
run(sparkSession: SparkSession) --> 调用CommandUtils.xxx 获取Statistics --> 保存到externalCatalog
- CommandUtils,获取Statistics的实现工具类
    - CommandUtils.calculateTotalSize获取sizeInBytes，基本就是用file system的接口去获取
    - CommandUtils.computeColumnStats,计算ColumnStat,是后面估算数据量的重要依据
        - 这里暂时占坑,可以看到直方图之类的都是在这个方法里算的
    - 从CommandUtils的引用上来看，spark的cbo统计信息是在用spark sql写入hive表时才更新的.
    - CommandUtils.updateTableStats更新cbo统计信息的时候，最后是externalCatalog.alterTableStats(db, table, newStats)，所以cbo统计信息是存在hive metastore的，应该是和hive共用一套统计信息.
```
- JoinSelection 
    - 在获得Statistics之后,JoinSelect根据joinType，joinKeys和left,right的sizeInByte来决定JoinExec
    - 基本看代码就可以看出来怎么选择join方式了.
    - ExtractEquiJoinKeys专门用来处理等值join.
        - 从joinCondition中获取等值的joinKey
        - 优先按照注释(hint)构建，如果没有hint，按顺序，根据joinType和两边的来构建JoinExec.
        - createJoinWithoutHint指定了无hint时的构造顺序

## 自适应执行Adaptive Execution
    - 位置：org.apache.spark.sql.execution.adaptive
    - 起点：preparations: Seq[Rule[SparkPlan]] = Seq(InsertAdaptiveSparkPlan(sparkSession, this))
        - InsertAdaptiveSparkPlan是一个Rule[SparkPlan]在spark plan中插入AdaptiveSparkPlanExec，用于在执行过程中，动态调整spark plan
        - InsertAdaptiveSparkPlan的逻辑是判断plan能否插入InsertAdaptiveSparkPlan，可以就在整个plan插入InsertAdaptiveSparkPlan，同时处理subqueries(in,exists这些)，因为InsertAdaptiveSparkPlan是leaf node,所以preparations下面的其他规则，包括处理subqueries的rule都不生效
- AdaptiveSparkPlanExec
    - 用于在spark plan执行的时候自适应调整spark plan
```
1、插入stage锚点QueryStageExec
- AdaptiveSparkPlanExec.doExecute首先会调用createQueryStages在spark plan中插入QueryStageExec。下游QueryStageExec会根据上游QueryStageExec的执行结果来调整QueryStageExec的spark plan child。
- 从createQueryStages可以看到，遇到Exchange节点，才会创建QueryStageExec，Exchange即是与shuffle有关的spark plan的基类,注意创建的QueryStageExec.resultOption = false，因为还没执行。
- createQueryStages返回结果是CreateStageResult,插入QueryStageExec的提前是Exchange节点的children plan调用createQueryStages返回的CreateStageResult.allChildStagesMaterialized都是true，但是插入QueryStageExec返回的CreateStageResult.allChildStagesMaterialize=false，因为QueryStageExec还没执行.
- 所有生成的QueryStageExec都按顺序放在CreateStageResult.newStages: Seq[QueryStageExec]
- QueryStageExec的生成是调用newQueryStage(e: Exchange)，newQueryStage会应用queryStageOptimizerRules去自适应优化spark plan，这是自适应优化的一部分逻辑
2、分stage执行spark plan
- while (!result.allChildStagesMaterialized),直至所有节点都materialized，自适应执行才结束
- 遍历createQueryStages返回的CreateStageResult.newStages，调用stage.materialize()，执行结果.如果success，stage.resultOption = Some(res).
- val logicalPlan = replaceWithQueryStagesInLogicalPlan(currentLogicalPlan, stagesToReplace),把currentLogicalPlan中的一部分plan替换成已经materialize的stage的logical plan
- val (newPhysicalPlan, newLogicalPlan) = reOptimize(logicalPlan),根据之前stage的结果，重新优化logical plan和生成spark plan，这里就是自适应执行的逻辑另一部分逻辑
- 再调用result = createQueryStages(currentPhysicalPlan),重新生成CreateStageResult.,继续while循环
3、自适应执行的优化逻辑(先占坑)
- reOptimize(logicalPlan)
    - DemoteBroadcastHashJoin,如果Partition结果非空的数量大于spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin，就添加一个hint，阻止BroadcastHashJoin
- queryStageOptimizerRules
    - ReduceNumShufflePartitions,动态调整shuffle partitions数量
```